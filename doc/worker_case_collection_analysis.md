# Worker-, Graph- und Scope-Recherche

## Prompt 1 – Konkrete Worker-Implementierungen

| Name | Datei | Zweck | Eingaben | Ausgaben/Seiteneffekte | Kategorie |
| --- | --- | --- | --- | --- | --- |
| `run_graph` Celery Task | `llm_worker/tasks.py` | Führt registrierte LangGraph-Flows oder den Hybrid-Scorer im Worker durch, um Web-Anfragen von Langlauf-Latency und Kostenbuchung zu entkoppeln. | Graphname, vorbereiteter `state`/`meta`, optionale Ledger-IDs sowie Scope-Felder wie `tenant_id`, `case_id`, `trace_id`. | Liefert aktualisierten Graph-State, Ergebnis-Payload und aggregierte Kostenmetriken; nutzt `track_ledger_costs` und ruft `get_graph_runner` bzw. `run_score_results`. | `agent`/`rag_search`【F:llm_worker/tasks.py†L13-L81】 |
| `ingest_raw` | `ai_core/tasks.py` | Persistiert hochgeladene Rohbytes im Object Store und erzeugt Content-Hash für Folgejobs. | Metadaten mit `external_id`, Dateiname, Bytes. | Schreibt Datei ab, reichert Meta um SHA256 an und gibt Speicherpfad zurück. | `ingestion`【F:ai_core/tasks.py†L354-L365】 |
| `extract_text` | `ai_core/tasks.py` | Dekodiert Rohbytes zu Textartefakten als Vorarbeit für PII-Maskierung und Chunking. | Meta sowie Pfad zur Rohdatei. | Speichert UTF‑8 Textdatei im Artefaktbaum und referenziert Pfad. | `ingestion`【F:ai_core/tasks.py†L368-L375】 |
| `pii_mask` | `ai_core/tasks.py` | Maskiert erkannte PII oder ersetzt Zahlen, falls Maskierung aktiv, um sensible Inhalte vor Downstream-Schritten zu schützen. | Meta und Pfad zur Textdatei. | Schreibt maskierten Text in neues Artefakt. | `ingestion`【F:ai_core/tasks.py†L378-L393】 |
| `_split_sentences` | `ai_core/tasks.py` | Hilfstask für Satzsegmentierung, liefert fallback auf Zeilen/Paragraphen zur weiteren Chunkbildung. | Textstring. | Gibt Liste segmentierter Strings zurück. | `ingestion`【F:ai_core/tasks.py†L396-L410】 |
| `embed` | `ai_core/tasks.py` | Lädt Chunk-Datei, normalisiert Text, ruft Embedding-Client in Batches auf und schreibt Vektoren samt Parents für spätere Upserts. | Meta (inkl. Tenant/Case/Collection) und Artefaktpfad zu Chunks. | Persistiert Vektor-JSON, meldet Telemetrie (`update_observation`) und protokolliert Kosten/Retry-Daten. | `ingestion`【F:ai_core/tasks.py†L1265-L1454】 |
| `upsert` | `ai_core/tasks.py` | Validiert Chunk-Metadaten, normalisiert Embeddings, prüft Dimensionen und schreibt in den tenant-spezifischen Vector-Client. | Meta, Pfad zum Embedding-Artefakt, optionaler Vector-Client. | Führt `upsert_chunks`, validiert Tenant-/Case-Felder und wirft bei Schema-Mismatch Fehler. | `ingestion`【F:ai_core/tasks.py†L1468-L1629】 |
| `ingestion_run` | `ai_core/tasks.py` | Schlanke Queueing-Hülle für das Ingestion-Run-API, protokolliert Aufträge mit Tenant/Case und Priorität. | Tenant, Case, Dokument-IDs, optional Trace. | Gibt Status „queued“ mit Zeitstempel zurück. | `ingestion`【F:ai_core/tasks.py†L1633-L1657】 |
| `run_ingestion_graph` | `ai_core/tasks.py` | Startet den Crawler-Ingestion-LangGraph inkl. Normalisierung, Trace-Aufbau und Cleanup von Payload-Artefakten. | Arbeitsstate (inkl. Rohdokument) und Meta. | Führt Graph aus, erzeugt NormalizedDocumentInput falls nötig, startet/endet Observability-Trace und entfernt temporäre Uploads. | `ingestion` LangGraph【F:ai_core/tasks.py†L1953-L2084】 |
| `process_document` | `ai_core/ingestion.py` | Vollständiger Dokumentlauf: Profilauflösung, Artefaktherstellung, Guardrails/Delta, Chunking, Embedding und Upsert pro Dokument. | Tenant, Case, Dokument-ID, Profil, optional Schema/Trace. | Persistiert Pipeline-State, sammelt Artefakte, ruft `resolve_ingestion_profile`, `make_fallback_external_id`, etc., schreibt Ergebnis mit Chunk-/Action-Zählern. | `ingestion`【F:ai_core/ingestion.py†L542-L960】 |
| `run_ingestion` | `ai_core/ingestion.py` | Orchestriert Batch-Ingestion: Partitioniert IDs, resolved Profil/Vektorspace, dispatcht `process_document` Gruppe, sammelt Resultate und Dead-Letters. | Tenant, Case, Dokumentliste, Profil, Run-/Trace-ID, Optionen. | Loggt Start/Ende, wartet auf Gruppenresultate, schreibt Dead Letters und fasst Insert/Replace/Skip zusammen. | `ingestion` orchestrator【F:ai_core/ingestion.py†L963-L1199】 |
| `record_dead_letter` | `ai_core/ingestion.py` | Dead-Letter-Verbraucher für fehlgeschlagene Dokumente, schreibt strukturierte Fehlermeldungen. | Payload mit Tenant/Case/Meta. | Loggt Fehler im dedizierten Queue-Kontext. | `maintenance`【F:ai_core/ingestion.py†L1318-L1370】 |
| `hard_delete` | `ai_core/rag/hard_delete.py` | Entfernt Dokumente physisch aus Vector-Store inkl. Vacuum/Reindex, nutzt separate `rag_delete` Queue. | Tenant-ID, Dokument-IDs, Reason, Ticket, optional Actor/Schema/Case. | Führt Vacuum/Reindex, Reset von Router/Client, protokolliert Umfang und Rückgabestats. | `maintenance`【F:ai_core/rag/hard_delete.py†L374-L420】 |
| `CollectionSearchGraph` | `ai_core/graphs/collection_search.py` | LangGraph, der Query-Expansion, parallele Websuche, Embedding-Ranking, HITL/Auto-Ingestion und Coverage-Verifikation für Collections koppelt. | Validierter `GraphInput` (Frage, `collection_scope`, Quality/Auto-Ingest Parameter) plus Kontext (`tenant_id`, `case_id`, Workflow). | Produziert Telemetrie pro Node, `transitions`, Search Snapshot und optional Ingestion/Coverage-Payloads. | `rag_search` LangGraph【F:ai_core/graphs/collection_search.py†L211-L915】 |
| `ExternalKnowledgeGraph` | `ai_core/graphs/external_knowledge_graph.py` | Websuche mit HITL-Review und Crawler-Ingestion für externe Knowledge-Sources, inkl. `run_until` Optionen. | `GraphInput` (Query, `collection_id`, HITL-Flags) + Kontext (Tenant/Case/Workflow). | Entscheidet zwischen auto-Selektion, Review und Ingestion, emittiert Transitions samt Meta. | `crawler`/`rag_search` LangGraph【F:ai_core/graphs/external_knowledge_graph.py†L31-L200】 |
| `CrawlerIngestionGraph` | `ai_core/graphs/crawler_ingestion_graph.py` | Minimales Orchestrierungs-Graph für dokumentbasierte Crawler-Läufe (Parser, Guardrails, Delta, Embedding, Persistenz). | Normalisierte Dokumentinputs inkl. Tenant/Case, Pipeline-Konfig, Parser-Adapter. | Baut Transitionen (Lifecycle, Delta, Guardrails), steuert Parser/Chunker und ruft Embedding/Upsert Services. | `ingestion` LangGraph【F:ai_core/graphs/crawler_ingestion_graph.py†L1-L120】 |
| `UploadIngestionGraph` | `ai_core/graphs/upload_ingestion_graph.py` | Fluss für manuelle Uploads inkl. Quarantäne, Dedupe, Normalisierung, Guardrails und Chunk/Embed Schritte. | Upload-Payload (`NormalizedDocument`), Feature-Flag Konfiguration, optional Guardrail/Scanner Adapter. | Übergibt Artefakte an Persistenz/Vektor-Pipeline, emittiert Standard-Transitions. | `ingestion` LangGraph【F:ai_core/graphs/upload_ingestion_graph.py†L1-L86】 |
| `retrieval_augmented_generation` Graph | `ai_core/graphs/retrieval_augmented_generation.py` | Produktionsgraph für RAG-Antworten: orchestriert Retrieve/Compose Nodes plus Guardrails für Sichtbarkeit. | ToolContext (Tenant/Case) und State mit Retrieval-/Compose-Parametern. | Aktualisiert State/Meta, normalisiert Snippets, liefert Compose-Ergebnis und optional Fehler. | `rag_search` LangGraph【F:ai_core/graphs/retrieval_augmented_generation.py†L1-L52】 |

**Scope & Queueing Takeaways:** Alle Worker in dieser Tabelle profitieren bereits von `ScopedTask` bzw. `with_scope_apply_async`, womit `tenant_id`, `case_id`, Trace und Session Salt automatisch auf jede Queue-Operation gelegt werden. Für Case-spezifische Jobs existiert mit `ingestion_run` eine dünne, jedoch verbreitete Eintrittsstelle, die über `run_ingestion_graph` die LangGraph-Orchestrierung bootet – neue Case Worker sollten sich genau daran anhängen statt eine eigene Queue zu definieren.【F:common/celery.py†L168-L220】【F:ai_core/tasks.py†L1633-L2084】

## Prompt 2 – search_collection_graph Struktur

- **Definition & Pfad:** `CollectionSearchGraph` lebt in `ai_core/graphs/collection_search.py` und kapselt alle Nodes für Collectionsuchen. `GraphInput` verlangt `question`, `collection_scope`, `quality_mode`, Limits und Auto-Ingestion-Parameter; Kontext wird via `GraphContextPayload` auf Tenant, Workflow, Case und optionale Run-/Ingestion-IDs normalisiert.【F:ai_core/graphs/collection_search.py†L211-L234】【F:ai_core/graphs/collection_search.py†L404-L464】
- **Erwartete Inputs:** Beim Start wird vorhandener State in `working_state`/`meta_state` übernommen. `graph_input` füllt Felder wie `question`, `collection_scope`, `purpose`, `max_candidates`, `auto_ingest_*`, während `GraphContextPayload` zwingend `tenant_id`, `workflow_id`, `case_id` sowie optionale `trace_id`, `run_id`, `ingestion_run_id` aus Meta/State zieht. Diese IDs landen in `meta_state["context"]` und erzeugen `telemetry["ids"]`.【F:ai_core/graphs/collection_search.py†L1125-L1179】【F:ai_core/graphs/collection_search.py†L404-L464】
- **Nodes:**
  - `k_generate_strategy` ruft den konfigurierten `strategy_generator` auf, schreibt die Query-Planung in den State und protokolliert Query-Anzahl, angewandte Policies sowie Quality-Mode in Telemetrie/Spans.【F:ai_core/graphs/collection_search.py†L536-L570】
  - `k_parallel_web_search` feuert alle Strategien via `WebSearchWorker`, sammelt Resultate, Fehler und Antwort-Metadaten pro Query, emittiert Events und speichert aggregierte Treffer, Fehlerlisten und Response-Meta im State.【F:ai_core/graphs/collection_search.py†L572-L691】
  - `k_embedding_rank` bildet Query+Purpose-Text, ruft `EmbeddingClient`, rechnet Cosine-Ähnlichkeit plus Heuristiken (`_calculate_generic_heuristics`) und schreibt `embedding_rank_score`, `embedding_similarity`, `heuristic_score` pro Resultat, inkl. Zeitmessung.【F:ai_core/graphs/collection_search.py†L693-L760】
  - `_k_hybrid_score` (über `hybrid_executor`) validiert Kandidaten, baut `ScoringContext` mit `collection_scope` und `case_id`, ruft Hybrid-Scorer, speichert Ranked/Top‑K/Coverage-Deltas, und führt optional HITL/AutoApprove Pfade weiter.【F:ai_core/graphs/collection_search.py†L840-L915】
  - `k_hitl_gate` persistiert Review-Payload, respektiert Deadlines, auto-approves nach Ablauf und ruft optional HITL-Gateway; Entscheidungen, Deadline und Auto-Approve-Status werden in `hitl_state`/Telemetrie abgelegt.【F:ai_core/graphs/collection_search.py†L917-L1019】
  - `k_trigger_ingestion` extrahiert genehmigte oder manuell hinzugefügte URLs, ruft den `ingestion_trigger`, schreibt dessen Meta in State und Telemetrie und emittiert Transition `ingest_triggered`.【F:ai_core/graphs/collection_search.py†L1022-L1064】
  - `k_verify_coverage` (optional) pollt Coverage per `coverage_verifier`, zählt erfolgreiche/fehlgeschlagene/pending URLs, berechnet Quoten und legt Zusammenfassung plus Transition `verified` ab.【F:ai_core/graphs/collection_search.py†L1066-L1098】
- **Ergebnisse:** `_build_result` konsolidiert `telemetry`, optionale HITL-/Ingestion-/Coverage-Blöcke sowie einen `search` Snapshot (inkl. Plan, Responses, Errors). Der finale Outcome unterscheidet Fälle wie `auto_ingest_triggered`, `auto_ingest_failed_quality_threshold` oder Standard `completed`, abhängig von Auto-Ingest-Filterung und Trigger-Erfolg. Alle Transitionen landen in `state["transitions"]` und Telemetrie enthält Node-Metas plus `ids` für Observability.【F:ai_core/graphs/collection_search.py†L516-L535】【F:ai_core/graphs/collection_search.py†L1240-L1358】

**Case/Collection Scope im Graph:** `collection_scope` ist nicht nur Filterinput, sondern Teil des `_GraphIds` Pakets und wird zusammen mit `case_id` in `meta_state["context"]`, in jedem `ScoringContext` sowie beim Triggern von Auto-Ingestion/Coverage geführt. Dadurch können künftige Case-Lifecycle-Worker diesen Graph ohne Sonderpfade aufrufen, weil sowohl Case- als auch Collection-Kontext bereits in allen Nodes verfügbar und telemetriert ist.【F:ai_core/graphs/collection_search.py†L404-L915】

### Verwandte Graphen

| Name | Pfad | Hauptunterschied |
| --- | --- | --- |
| `ExternalKnowledgeGraph` | `ai_core/graphs/external_knowledge_graph.py` | Ebenfalls Websuche + HITL, aber fokussiert auf eine konkrete `collection_id`, bietet `run_until`-Stopps, prüft Blocked Domains und triggert den Crawler über einen Adapter statt automatischer URL-Listen wie im Collection-Graph. Kontextvalidierung erzwingt `tenant_id/workflow_id/case_id`.【F:ai_core/graphs/external_knowledge_graph.py†L31-L200】 |
| `RetrievalAugmentedGeneration` | `ai_core/graphs/retrieval_augmented_generation.py` | RAG-Antwortfluss mit generischen Retrieve/Compose-Nodes statt Websuche; Inputs sind `ToolContext` (inkl. Case) und Compose State, Fokus liegt auf Snippet-Normalisierung und Guardrail-Flags, keine Web- oder Ingestion-Schritte. |【F:ai_core/graphs/retrieval_augmented_generation.py†L1-L52】 |
| `UploadIngestionGraph` | `ai_core/graphs/upload_ingestion_graph.py` | Prozessiert Uploads (Quarantäne→Dedupe→Persist→Chunk/Embed) ohne Websuche/HITL; dient der Einsteuerung neuer Dokumente inkl. Guardrail-/Delta-Prüfung und `run_until`-Stages. |【F:ai_core/graphs/upload_ingestion_graph.py†L1-L76】 |

## Prompt 3 – Domänenmodelle für Dokumente, Collections, Tenants & Nutzer

1. **`DocumentLifecycleState` (Django Modell, Persistenz) – `documents/models.py`:** speichert pro Tenant/Workflow/Document den aktuellen Zustand, Trace-/Run-/Ingestion-IDs, Reason und Policy-Events; Unique-Constraint stellt nur einen Lifecycle-Datensatz je Kombination sicher und Indizes ermöglichen Abfragen pro Tenant/Workflow.【F:documents/models.py†L6-L37】
2. **`DocumentIngestionRun` (Django Modell, Persistenz) – `documents/models.py`:** protokolliert den letzten Ingestionlauf pro Tenant/Case mitsamt `collection_id`, Status, Queue/Start/Ende, Dauer, Insert/Replace/Skip-Zählern, Dokumentlisten, Trace und Profilinformationen; Unique-Constraint liegt auf `(tenant_id, case)`.【F:documents/models.py†L39-L78】
3. **`DocumentRef` (Pydantic Domain-Objekt) – `documents/contracts.py`:** referenziert Dokumente mit `tenant_id`, `workflow_id`, `document_id`, optional `collection_id` und `version`, inklusive Normalisierung von Tenant/Workflow-IDs und UUID-Koerzierung; dient als internes Transferobjekt zwischen Parsern, Repositories und Services.【F:documents/contracts.py†L9-L40】
4. **`DocumentMeta` (Pydantic Metadata Contract) – `documents/contracts.py`:** hält Titel, Sprache, Tags, Ursprung, Crawl-Timestamp, `external_ref`, Parser-Stats, Pipeline-Overrides usw.; normalisiert Tenant/Workflow, validiert BCP‑47-Sprachen und Tag-Limits. Wird als API/DTO zwischen Parsern und Persistenz genutzt.【F:documents/contracts.py†L350-L410】
5. **`NormalizedDocumentInputV1` (API/Ingress Schema) – `documents/contracts.py`:** Inputvertrag für Rohdokumente mit Feldern wie `tenant_id`, optionaler `workflow_id`, `collection_id`, `case_id`, Tags, Metadaten und Blob-Descriptor; validiert Strings und erlaubt Aufrufern, Collection/Case-Kontext bereits bei Uploads zu setzen.【F:documents/contracts.py†L738-L808】
6. **`DocumentProcessingMetadata` (Pydantic Transferobjekt) – `documents/pipeline.py`:** unveränderliche Metadata, die entlang der Pipeline getragen wird; enthält `tenant_id`, optional `collection_id`/`case_id`, `workflow_id`, `document_id`, Version, Source, Timestamps und Trace-/Span-IDs; Validatoren normalisieren Tenant/Workflow und optional Strings.【F:documents/pipeline.py†L229-L299】
7. **`NormalizedDocument` (Pydantic Domain-Modell) – `documents/contracts.py`:** repräsentiert voll normalisierte Dokumente inkl. Meta/Blob; kann `case_id`, Tags, Language etc. aus `metadata` ableiten, falls nicht direkt gesetzt, wodurch Case-/Collection-Informationen in Persistenz und nachgelagerte Prozesse gespiegelt werden.【F:documents/contracts.py†L920-L1002】
8. **`Tenant` (Django Modell) – `customers/models.py`:** Multi-Tenant Rootobjekt basierend auf `TenantMixin`, enthält Laufzeitdaten (Name, Billing, Flags) sowie PII-Konfiguration (`pii_mode`, `pii_policy`, diverse Toggles) und generiert eigene Schemas. Dient als Persistenzmodell für Mandanten und deren RAG-Datenräume.【F:customers/models.py†L6-L25】
9. **`UserProfile` (Django Modell) – `profiles/models.py`:** Zuordnung von Benutzer*innen zu Rollen (Admin, Legal, BR, Manager, Guest) inkl. Aktiv-Flag; dient dem Zugriffsmodell auf AI/RAG-Funktionalitäten mit direktem Bezug zu Tenant-Scoped Dokumenten/Collections. |【F:profiles/models.py†L5-L18】

**Design-Fazit:** `collection_id` ist im gesamten Pfad – vom Upload (`NormalizedDocumentInputV1`) über Pipeline (`DocumentProcessingMetadata`) bis zur Persistenz (`DocumentIngestionRun`) – technisch etabliert und wird downstream im `vector_client` respektiert. Künftige Konzepte wie „Akte/Mappe“ sollten daher als logische Ebene oberhalb dieses Felds modelliert werden, statt den Vector Store umzubauen.【F:documents/contracts.py†L738-L1002】【F:documents/pipeline.py†L229-L299】【F:documents/models.py†L39-L78】【F:ai_core/rag/vector_client.py†L640-L700】

## Prompt 4 – Existiert ein Case-ähnliches Modell?

- **Persistenzmodell vorhanden:** Die `cases`-App definiert ein vollwertiges `Case`-Modell mit Tenant-Relation, `external_id` (der bisher als Kontext-String genutzte Wert), Status (`open|closed`), frei wählbarer `phase`, flexiblem `metadata` JSON sowie Zeitstempeln inkl. `closed_at`. Damit ist Case kein Phantom, sondern eine persistente Entität.【d9ac29†L1-L3】【438136†L3-L4】
- **Event-Sourcing:** `CaseEvent` speichert jede Zustandsänderung mit `event_type`, `source`, optionalem `graph_name`, `ingestion_run`, `workflow_id`, `collection_id`, `trace_id` und frei strukturiertem `payload` als Audit-Log und Anker für die State Machine.【d9ac29†L1-L3】
- **Lifecycle-Definition:** In `cases/contracts.py` steckt `CaseLifecycleDefinition` mit einer Default-Konfiguration: Phasen `intake → evidence_collection → external_review → search_completed` und Events, die diese Phasen treiben (z. B. `ingestion_run_started/completed`, `collection_search:ingest_triggered`, `collection_search:hitl_pending`, `collection_search:verified`).【83c542†L2-L6】
- **State-Machine-Logik:** `apply_lifecycle_definition` rekonstruiert den aktuellen Status aus der Event-Historie, indem sie alle `CaseEvent`-Einträge chronologisch abarbeitet. `update_case_from_collection_search` transformiert den LangGraph-State (Transitions wie `collection_search:ingest_triggered` oder `:verified`) in CaseEvents und aktualisierte Phasen.【1ba768†L2-L4】【83c542†L2-L6】
- **Integrationspunkte:** `record_ingestion_case_event` bindet Ingestion-Runs an das Case-Lifecycle an, indem pro `DocumentIngestionRun`-Status passende `CaseEvent`-Einträge erzeugt werden; `emit_case_lifecycle_for_collection_search` erlaubt es, CollectionSearchGraph-Läufe als Events einzuspielen, ohne den Graph selbst anzupassen.【1ba768†L1-L4】
- **Konsequenz für neue Features:** Case hat bereits ein Event-getriebenes Lifecycle-Subsystem mit Bridge-Funktionen zu Ingestion und CollectionSearch. Neue Case-Worker sollten daher auf diesen Event-Strom (Ingestion- und Graph-Events) aufsetzen, statt Case erneut als String-Kontext zu modellieren.

## Prompt 5 – Konkrete Verwendung von `case_id`

### Domänen-/ORM-Modelle
- `Case` und `CaseEvent` in der `cases`-App bilden ein persistentes Case-Objekt (Status, Phase, Metadata, Zeitstempel) plus Event-Historie mit Typ/Quelle/Graph/Trace/Workflow/Collection-Bezug, sodass jede Zustandsänderung nachvollziehbar bleibt.【d9ac29†L1-L3】【438136†L3-L4】
- `DocumentIngestionRun.case` speichert Case-Strings pro Tenant, was den Statusverlauf eines Ingestion-Laufs referenziert und für Statusabfragen genutzt wird.【F:documents/models.py†L39-L78】
- `DocumentProcessingMetadata.case_id` und `NormalizedDocumentInputV1.case_id` sorgen dafür, dass Dokument- und Pipelineobjekte Case-Kontext tragen; `NormalizedDocument` übernimmt `case_id` auch rückwirkend aus Metadaten, falls nötig.【F:documents/pipeline.py†L229-L299】【F:documents/contracts.py†L738-L808】【F:documents/contracts.py†L920-L1002】

### API Layer
- `_prepare_request` im AI-Core verlangt den `X-Case-ID` Header, validiert dessen Format und legt `case_id` in `meta`, `request.META` und Logging-Context für alle Folgeoperationen; ohne validen Header wird ein 400-Fehler zurückgegeben.【F:ai_core/views.py†L542-L668】
- `RagIngestionStatusView` ruft `get_latest_ingestion_run` mit `tenant_id` und `case_id` auf, sodass Statusendpunkte strikt pro Case antworten; bei fehlendem Run erfolgt ein 404 mit Hinweis auf Case-spezifische Historie.【F:ai_core/views.py†L2154-L2169】
- `assert_case_active` ist aktuell ein NOP, aber zeigt, dass Case-Prüfungen auf API-Ebene vorgesehen sind.【F:ai_core/views.py†L192-L199】

### Worker & Hintergrundjobs
- `process_document` setzt `meta["case_id"] = case` bevor Parser/Chunker laufen, sodass alle Artefakte und Logs Case-kontextualisiert sind.【F:ai_core/ingestion.py†L626-L633】
- `run_ingestion` loggt Tenant/Case-Kombinationen bei Start/Ende und gibt Case-bezogene Dead-Letters weiter, womit Batch-Läufe pro Business Case nachvollziehbar bleiben.【F:ai_core/ingestion.py†L986-L1175】
- `embed` schreibt Case-IDs in Observability (`session_id`) und Logs, wodurch Embedding-Batches Case-bezogen ausgewertet werden können; `upsert` loggt Case-Kontext bei Parent-Debugmeldungen. 【F:ai_core/tasks.py†L1292-L1303】【F:ai_core/tasks.py†L1489-L1497】
- `run_ingestion_graph` rekonstruiert Case-IDs aus Trace/State, injiziert sie in `metadata` und startet Observability-Traces mit `session_id=case_id`, sodass der gesamte LangGraph-Lauf Case-korrelierbar bleibt.【F:ai_core/tasks.py†L1953-L2069】
- `ScopedTask` Basis propagiert `(tenant_id, case_id, session_salt)` als Scope; `with_scope_apply_async` füllt fehlende Case-Werte nach, damit alle Celery-Tasks denselben Case-Kontext haben.【F:common/celery.py†L168-L220】
- `llm_worker.tasks.run_graph` akzeptiert `case_id` (auch wenn intern nicht verwendet), damit Scope-Middleware den Kontext injizieren kann.【F:llm_worker/tasks.py†L13-L45】

### RAG & Retrieval
- `WebSearchContext` fordert `case_id` als Pflichtfeld, sodass externe Suchen jedes Worker-Call dem passenden Business Case zuordnen.【F:ai_core/tools/web_search.py†L103-L123】
- `CollectionSearchGraph` speichert `case_id` in `_GraphIds`, trägt es in Telemetrie/Context-Snapshots ein und reicht es an Search-, Hybrid-, HITL- und Ingestion-Nodes weiter.【F:ai_core/graphs/collection_search.py†L404-L464】【F:ai_core/graphs/collection_search.py†L572-L915】
- `ai_core/rag/vector_client` normalisiert `case_id` Filter (`normalized_filters["case_id"] = ...`) und baut SQL-Klauseln, die `c.metadata ->> 'case_id'` berücksichtigen, inklusive Kombination mit Collection-Filtern und Legacy-Doc-Class, womit Retrievals Cases auf Chunk-Ebene filtern können.【F:ai_core/rag/vector_client.py†L657-L675】【F:ai_core/rag/vector_client.py†L1773-L1793】

## Prompt 6 – Verwendung von `collection_id`

### Modelle & Contracts
- `DocumentIngestionRun` persistiert `collection_id`, sodass Lauf-Historien einem logischen Dokumentbereich zuordenbar bleiben.【F:documents/models.py†L39-L78】
- `DocumentRef` und `DocumentProcessingMetadata` tragen `collection_id` optional mit, wodurch Repository-Operationen, Lifecycle-Einträge und Observability denselben Scope nutzen.【F:documents/contracts.py†L9-L40】【F:documents/pipeline.py†L229-L299】
- `NormalizedDocumentInputV1` erlaubt Caller-seitig, `collection_id` direkt im Upload zu setzen; das Feld wandert so bis in Normalization/Repository-Schichten. |【F:documents/contracts.py†L738-L808】

### Services & Utilities
- `process_document` übernimmt `normalized_document.ref.collection_id` und schreibt ihn in das Meta des Artefaktlaufs, wodurch jeder Embedding/Upsert-Schritt weiß, welche Collection betroffen ist.【F:ai_core/ingestion.py†L626-L639】
- `ai_core/tasks.embed` sendet `collection_id` mit in Observability-Metadata, um Embedding-Batches einem Collection Scope zuzuordnen und Filterungen nachzuvollziehen.【F:ai_core/tasks.py†L1292-L1301】
- `_prepare_request` im API-Layer liest `X-Collection-ID`, ergänzt `meta` sowie `request.META` und reichert den Log-Kontext an; wenn der Body keinen Wert enthält, kann dieser Header später in Payloads übernommen werden (siehe `_prepare_request` + `start_ingestion_run`).【F:ai_core/views.py†L542-L668】【F:ai_core/views.py†L2140-L2148】

### RAG Zugriff
- `ai_core/rag/vector_client` normalisiert `collection_id` Einzelwerte und Listen, merkt sich, ob ein Single-Filter aktiv ist, und kombiniert `case_id`/`collection_id` Bedingungen im SQL-Generator (z. B. `c.collection_id = ANY(%s::uuid[])` oder gemischte Case/Collection-Klauseln). Dadurch lassen sich Retrievals exakt auf Collections einschränken.【F:ai_core/rag/vector_client.py†L640-L700】【F:ai_core/rag/vector_client.py†L1773-L1797】
- `CollectionSearchGraph` verwendet `collection_scope` (semantisch Collection-ID oder Alias) beim Aufbau von `_GraphIds`, Telemetrie und beim Triggern von Auto-Ingestion/Coverage, wodurch Search/Approval-Läufe eindeutig einer Collection zugeordnet werden.【F:ai_core/graphs/collection_search.py†L404-L464】【F:ai_core/graphs/collection_search.py†L1022-L1078】

