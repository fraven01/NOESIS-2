# HITL (Human-In-The-Loop) Crawler Design

## Overview

Add manual review step between fetch and ingestion.

---

## ðŸ”„ Two-Phase Flow

### Phase 1: Fetch & Preview (HITL)

```
User submits URLs
    â†“
Celery Task: fetch_for_review
    â†“
Fetch content (no persistence yet)
    â†“
Extract preview (title, snippet, metadata)
    â†“
Store in temporary cache (Redis/DB)
    â†“
Return preview_session_id
    â†“
UI displays previews
    â†“
User reviews and selects
    â†“
User clicks "Ingest Selected"
```

### Phase 2: Ingest Selected

```
User confirms selection
    â†“
API receives selected URLs
    â†“
Celery Task: ingest_urls (from cache)
    â†“
Process selected URLs (Mode: RAG/Archive/Ephemeral)
    â†“
Return results
```

---

## ðŸŽ¨ API Design

### Step 1: Create Review Session

```python
POST /api/v1/crawler/fetch-for-review
{
  "urls": ["https://example.com", "https://example.org"],
  "session_metadata": {
    "source": "intelligent_search",
    "query": "neural networks"
  }
}

Response:
{
  "session_id": "review-session-uuid",
  "task_id": "celery-task-id",
  "urls_queued": 2,
  "status": "fetching"
}
```

### Step 2: Get Preview Results

```python
GET /api/v1/crawler/review-session/{session_id}

Response:
{
  "session_id": "review-session-uuid",
  "status": "completed",  // "fetching" | "completed" | "failed"
  "previews": [
    {
      "url": "https://example.com",
      "title": "Neural Networks Explained",
      "snippet": "A neural network is a method in artificial...",
      "content_type": "text/html",
      "size_bytes": 45621,
      "fetch_timestamp": "2025-12-11T08:24:00Z",
      "metadata": {
        "language": "en",
        "author": "John Doe",
        "published_date": "2024-01-15"
      },
      "thumbnail_url": "/api/v1/crawler/thumbnail/{preview_id}",  // optional
      "status": "fetched"
    },
    {
      "url": "https://example.org",
      "title": null,
      "snippet": null,
      "status": "failed",
      "error": "Connection timeout"
    }
  ],
  "expires_at": "2025-12-11T09:24:00Z"  // 1 hour TTL
}
```

### Step 3: Ingest Selected

```python
POST /api/v1/crawler/ingest
{
  "session_id": "review-session-uuid",
  "selected_urls": ["https://example.com"],  // User selected subset
  "mode": "rag",
  "collection_id": "uuid-here",
  "embedding_profile": "standard"
}

Response:
{
  "task_id": "celery-task-id",
  "urls_queued": 1,
  "mode": "rag"
}
```

---

## ðŸ—„ï¸ Data Model

### ReviewSession

```python
@dataclass
class ReviewSession:
    session_id: UUID
    tenant_id: str
    case_id: str | None
    urls: list[str]
    status: Literal["fetching", "completed", "failed", "expired"]
    created_at: datetime
    expires_at: datetime
    metadata: dict[str, Any]
```

### FetchedPreview

```python
@dataclass
class FetchedPreview:
    preview_id: UUID
    session_id: UUID
    url: str
    status: Literal["fetched", "failed"]
    
    # If fetched:
    title: str | None
    snippet: str  # First 500 chars
    content_type: str
    size_bytes: int
    content_cache_key: str  # Redis key for full content
    metadata: dict[str, Any]
    thumbnail_data: bytes | None
    
    # If failed:
    error: str | None
    
    fetch_timestamp: datetime
```

---

## ðŸ”§ Implementation

### Celery Task: fetch_for_review

```python
@shared_task
def fetch_for_review(
    session_id: str,
    urls: list[str],
    tenant_id: str,
    case_id: str | None = None,
) -> dict:
    """
    Fetch URLs and create previews for manual review.
    Does NOT persist to repository.
    """
    session = ReviewSession.objects.get(pk=session_id)
    session.status = "fetching"
    session.save()
    
    previews = []
    for url in urls:
        try:
            # Fetch content
            fetcher = HttpFetcher()
            content = fetcher.fetch(url, timeout=10)
            
            # Parse for preview
            parser = get_parser_for_content_type(content.content_type)
            parsed = parser.parse(content.body)
            
            # Extract snippet (first 500 chars)
            snippet = parsed.primary_text[:500] if parsed.primary_text else ""
            
            # Cache full content in Redis (1 hour TTL)
            cache_key = f"fetch_preview:{session_id}:{url}"
            redis_client.setex(
                cache_key,
                3600,  # 1 hour
                content.body
            )
            
            # Create preview
            preview = FetchedPreview(
                preview_id=uuid4(),
                session_id=session_id,
                url=url,
                status="fetched",
                title=parsed.title or _extract_title_from_url(url),
                snippet=snippet,
                content_type=content.content_type,
                size_bytes=len(content.body),
                content_cache_key=cache_key,
                metadata={
                    "language": parsed.content_language,
                    # ... other metadata
                },
                fetch_timestamp=datetime.now(timezone.utc),
            )
            previews.append(preview)
            
        except Exception as e:
            # Failed fetch
            preview = FetchedPreview(
                preview_id=uuid4(),
                session_id=session_id,
                url=url,
                status="failed",
                error=str(e),
                fetch_timestamp=datetime.now(timezone.utc),
            )
            previews.append(preview)
    
    # Save previews to DB
    FetchedPreview.objects.bulk_create(previews)
    
    # Update session
    session.status = "completed"
    session.save()
    
    return {
        "session_id": str(session_id),
        "previews_created": len(previews),
        "failed": sum(1 for p in previews if p.status == "failed"),
    }
```

### Celery Task: ingest_selected

```python
@shared_task
def ingest_selected(
    session_id: str,
    selected_urls: list[str],
    mode: Literal["rag", "archive", "ephemeral"],
    tenant_id: str,
    **kwargs
) -> dict:
    """
    Ingest selected URLs from review session.
    Uses cached content from fetch_for_review.
    """
    session = ReviewSession.objects.get(pk=session_id)
    previews = FetchedPreview.objects.filter(
        session_id=session_id,
        url__in=selected_urls,
        status="fetched"
    )
    
    results = {
        "completed": [],
        "failed": {},
        "documents_created": [],
    }
    
    for preview in previews:
        try:
            # Retrieve cached content
            content_body = redis_client.get(preview.content_cache_key)
            if content_body is None:
                raise ValueError("Cached content expired")
            
            # Process based on mode
            doc_id = _ingest_from_preview(
                preview=preview,
                content_body=content_body,
                mode=mode,
                tenant_id=tenant_id,
                **kwargs
            )
            
            results["completed"].append(preview.url)
            if doc_id:
                results["documents_created"].append(doc_id)
                
        except Exception as e:
            results["failed"][preview.url] = str(e)
    
    # Clean up cache
    for preview in previews:
        redis_client.delete(preview.content_cache_key)
    
    return results
```

---

## ðŸŽ¨ UI/UX Flow

### 1. Fetch & Preview Screen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Review Fetched Content (2 of 2 fetched)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚ â˜‘ Neural Networks Explained                    â”‚
â”‚   https://example.com                           â”‚
â”‚   A neural network is a method in artificial... â”‚
â”‚   HTML â€¢ 45 KB â€¢ Published: 2024-01-15         â”‚
â”‚   [Preview Full] [Remove]                       â”‚
â”‚                                                 â”‚
â”‚ â˜ Deep Learning Tutorial (FAILED)              â”‚
â”‚   https://example.org                           â”‚
â”‚   âŒ Connection timeout                         â”‚
â”‚   [Retry] [Remove]                              â”‚
â”‚                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Select All] [Deselect All]                    â”‚
â”‚                                                 â”‚
â”‚ Mode: â— RAG  â—‹ Archive  â—‹ Ephemeral           â”‚
â”‚ Collection: [Neural Networks Research â–¼]       â”‚
â”‚                                                 â”‚
â”‚ [Cancel]              [Ingest Selected (1)] â†’  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Preview Full Content (Modal)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Preview: Neural Networks Explained             Ã— â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ URL: https://example.com                          â”‚
â”‚ Type: text/html â€¢ 45 KB                          â”‚
â”‚                                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ # Neural Networks Explained                    â”‚ â”‚
â”‚ â”‚                                                â”‚ â”‚
â”‚ â”‚ A neural network is a method in artificial     â”‚ â”‚
â”‚ â”‚ intelligence that teaches computers to process â”‚ â”‚
â”‚ â”‚ data in a way that is inspired by the human    â”‚ â”‚
â”‚ â”‚ brain...                                       â”‚ â”‚
â”‚ â”‚                                                â”‚ â”‚
â”‚ â”‚ [... full parsed content ...]                  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                   â”‚
â”‚ [Close]                  [Include in Ingestion]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â±ï¸ Session Management

### TTL Strategy

```python
# Review sessions expire after 1 hour
REVIEW_SESSION_TTL = 3600  # seconds

# Cached content expires after 1 hour
CONTENT_CACHE_TTL = 3600  # seconds

# Cleanup job runs every 15 minutes
@periodic_task(run_every=timedelta(minutes=15))
def cleanup_expired_sessions():
    expired = ReviewSession.objects.filter(
        status__in=["completed", "failed"],
        created_at__lt=datetime.now() - timedelta(hours=1)
    )
    
    for session in expired:
        # Delete associated previews
        FetchedPreview.objects.filter(session_id=session.session_id).delete()
        
        # Delete session
        session.delete()
```

---

## ðŸ” Security Considerations

### 1. Session Ownership

```python
# Verify user owns session before viewing
def get_review_session(session_id: UUID, user: User):
    session = ReviewSession.objects.get(pk=session_id)
    if session.tenant_id != user.tenant_id:
        raise PermissionDenied()
    return session
```

### 2. Content Sanitization

```python
# Sanitize HTML previews for XSS
from bleach import clean

def sanitize_snippet(html: str) -> str:
    return clean(
        html,
        tags=["p", "br", "strong", "em"],
        attributes={},
        strip=True
    )
```

### 3. Rate Limiting

```python
# Limit review sessions per user
@rate_limit(key="user", rate="10/hour")
def create_review_session(request):
    ...
```

---

## ðŸ“Š Database Schema

```sql
-- Review Sessions
CREATE TABLE crawler_review_session (
    session_id UUID PRIMARY KEY,
    tenant_id VARCHAR(255) NOT NULL,
    case_id VARCHAR(255),
    status VARCHAR(20) NOT NULL,  -- fetching, completed, failed, expired
    created_at TIMESTAMP NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    metadata JSONB,
    
    INDEX idx_tenant_created (tenant_id, created_at),
    INDEX idx_expires (expires_at)
);

-- Fetched Previews
CREATE TABLE crawler_fetched_preview (
    preview_id UUID PRIMARY KEY,
    session_id UUID NOT NULL REFERENCES crawler_review_session(session_id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    status VARCHAR(20) NOT NULL,  -- fetched, failed
    
    -- If fetched:
    title TEXT,
    snippet TEXT,
    content_type VARCHAR(100),
    size_bytes INTEGER,
    content_cache_key VARCHAR(255),
    metadata JSONB,
    thumbnail_data BYTEA,
    
    -- If failed:
    error TEXT,
    
    fetch_timestamp TIMESTAMP NOT NULL,
    
    INDEX idx_session (session_id),
    INDEX idx_url (url)
);
```

---

## ðŸŽ¯ Benefits of HITL

1. **Quality Control** âœ…
   - Review content before ingestion
   - Filter out irrelevant results
   - Catch fetch errors early

2. **Cost Optimization** âœ…
   - Skip embedding for rejected content
   - No wasted vector storage
   - User selects only relevant docs

3. **Metadata Enrichment** âœ…
   - User can add tags during review
   - Correct auto-detected metadata
   - Improve search quality

4. **Transparency** âœ…
   - User sees what's being ingested
   - Trust in the system
   - Better UX

---

## ðŸ”„ Updated Flow Diagram

```
User Input: URLs
    â†“
[API] Create Review Session
    â†“
[Celery] fetch_for_review
    â”œâ”€ Fetch URL 1 â†’ Parse â†’ Cache â†’ Preview âœ…
    â”œâ”€ Fetch URL 2 â†’ Parse â†’ Cache â†’ Preview âœ…
    â””â”€ Fetch URL 3 â†’ Failed âŒ
    â†“
[DB] Store Previews (temp, 1h TTL)
    â†“
[UI] Display Previews
    â†“
USER REVIEWS â† ðŸ‘¤ HUMAN IN THE LOOP
    â”œâ”€ Select URL 1 âœ…
    â”œâ”€ Reject URL 2 âŒ
    â””â”€ Skip URL 3 (failed) âŒ
    â†“
[API] Ingest Selected
    â†“
[Celery] ingest_selected (from cache)
    â”œâ”€ URL 1: Mode=RAG â†’ Parse â†’ Persist â†’ Embed âœ…
    â””â”€ Clean up cache
    â†“
[UI] Show Results
```

---

## ðŸ’¡ Future Enhancements

### Batch Actions

- "Select All"
- "Deselect All"
- "Select by criteria" (e.g., all HTML, all > 10KB)

### Metadata Editing

- Edit title before ingestion
- Add/remove tags
- Set custom metadata

### Preview Improvements

- Syntax highlighting for code
- Thumbnail generation for PDFs
- Table of contents for long documents

### Collaboration

- Share review session with team
- Collaborative selection
- Comments/annotations

---

## ðŸ¤” Open Questions

1. **Cache Backend**: Redis or Database?
   - Recommendation: Redis (faster, built-in TTL)

2. **Thumbnail Generation**: Client-side or server-side?
   - Recommendation: Server-side (consistent quality)

3. **Diff Support**: Show changes if URL was previously ingested?
   - Recommendation: Yes, but Phase 2

4. **Bulk Operations**: Max URLs per session?
   - Recommendation: 100 URLs, auto-split larger batches

---

**Ready to implement?** ðŸš€

This HITL design adds ~1 day to timeline (now 6 days total) but provides massive UX improvement!
