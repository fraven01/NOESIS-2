# Reality Check: /rag-tools/ Components - No Hallucinations

## ğŸ¯ Objective

Verify what ACTUALLY exists in `/rag-tools/` and ensure our crawler redesign doesn't break it.

**Date**: 2025-12-11  
**Method**: Code inspection, no assumptions

---

## âœ… VERIFIED: What EXISTS in /rag-tools/

### Tab 1: Web Search (`#tab-search`)

**Location**: `theme/templates/theme/rag_tools.html:92-201`

**Features** (Line-by-line verified):

1. **Search Type Selection** (Lines 101-116):
   - âœ… External Knowledge (radio)
   - âœ… Collection Search (radio)

2. **Form Fields** (Lines 118-178):
   - âœ… Query (text input, required)
   - âœ… Purpose (text input, required for collection search)
   - âœ… Mode (select: `live` or `archive`)
   - âœ… Workflow ID (text input, default: `web-search`)
   - âœ… Collection ID (text input with datalist)
   - âœ… Quality Mode (select, hidden unless collection search)
   - âœ… Auto Ingest (checkbox, hidden unless collection search)

3. **Submit Button** (Line 181-184):
   - âœ… "Search" button
   - âœ… HTMX POST to `{% url 'web-search' %}`
   - âœ… Target: `#web-search-results`

4. **Result Display** (Lines 192-199):
   - âœ… `#web-search-results` div
   - âœ… Shows "Enter a query to see results here." placeholder

5. **Ingestion Status Panel** (Line 189):
   - âœ… `#ingestion-status-panel` div
   - âœ… Target for "Ingest Selected" results

**Backend Handler**: `theme/views.py:web_search_and_ingest()`

**Current Flow**:

```
User fills form â†’ clicks "Search"
    â†“
HTMX POST to /web-search
    â†“
web_search_and_ingest() view
    â”œâ”€ If search_type == "external_knowledge":
    â”‚  â””â”€ Calls ExternalKnowledgeGraph
    â”‚     â””â”€ Returns search results with "Ingest Selected" button
    â”‚
    â””â”€ If search_type == "collection_search":
       â””â”€ Calls CollectionSearchGraph
          â””â”€ Returns search results with optional auto-ingest
```

**"Ingest Selected" Flow** (Verified in `theme/views.py:850-950`):

```
User clicks "Ingest Selected" on search results
    â†“
HTMX with selected URLs
    â†“
web_search_ingest_selected() view (Line 850+)
    â”œâ”€ Extract URLs from request
    â”œâ”€ Build crawl_payload:
    â”‚  {
    â”‚    "urls": [...],
    â”‚    "workflow_id": "web-search-ingestion",
    â”‚    "collection_id": "...",
    â”‚    "mode": "live" or "archive",
    â”‚  }
    â”œâ”€ Create synthetic HttpRequest
    â””â”€ Calls crawl_selected(crawl_request) â† THIS IS THE INTEGRATION POINT!
       â†“
    Returns ingestion status to #ingestion-status-panel
```

**KEY FINDING**:

- âœ… Web Search â†’ Ingest Selected calls `crawl_selected()`
- âœ… This is the integration point we MUST NOT BREAK!

---

### Tab 2: Crawler (`#tab-crawler`)

**Location**: `theme/templates/theme/rag_tools.html:203-269`

**Features** (Verified):

1. **Form Fields** (Lines 210-253):
   - âœ… Origin URL (url input)
   - âœ… Additional Origins (textarea, one per line)
   - âœ… Mode (select: `live` or `archive`)
   - âœ… Workflow ID (text input, default: `crawler-manual`)
   - âœ… Fetch Content (checkbox)
   - âœ… Dry Run (checkbox)
   - âœ… Shadow Mode (checkbox)

2. **Submit Button** (Lines 256-259):
   - âœ… "Start Crawl" button
   - âœ… HTMX POST to `{% url 'crawler-submit' %}`
   - âœ… Target: `#crawler-status-area`

3. **Status Display** (Lines 263-267):
   - âœ… `#crawler-status-area` div
   - âœ… Shows "Crawler status will appear here." placeholder

**Backend Handler**: `theme/views.py:crawler_submit_view()`

**Current Flow**:

```
User fills form â†’ clicks "Start Crawl"
    â†“
HTMX POST to /crawler-submit
    â†“
crawler_submit_view()
    â”œâ”€ Parse origin_url and origin_urls
    â”œâ”€ Build CrawlerRunRequest payload
    â””â”€ Calls run_crawler_runner() â† Different from Web Search!
       â†“
    Returns crawler status
```

**KEY FINDING**:

- âœ… Crawler Tab does NOT use `crawl_selected()`
- âœ… Uses `run_crawler_runner()` directly
- âš ï¸ Different code path than Web Search!

---

### Tab 3: Ingestion (`#tab-ingestion`)

**Location**: `theme/templates/theme/rag_tools.html:271-305`

**Features** (Verified):

1. **Form Fields** (Lines 278-289):
   - âœ… Document IDs (textarea, JSON list or comma separated)
   - âœ… Embedding Profile (text input)

2. **Submit Button** (Lines 292-295):
   - âœ… "Run Ingestion" button
   - âœ… HTMX POST to `{% url 'ingestion-submit' %}`
   - âœ… Target: `#ingestion-response`

3. **Response Display** (Lines 299-303):
   - âœ… `#ingestion-response` div
   - âœ… Shows "Ingestion response will appear here." placeholder

**Backend Handler**: `theme/views.py:ingestion_submit_view()`

**Current Flow**:

```
User enters document IDs â†’ clicks "Run Ingestion"
    â†“
HTMX POST to /ingestion-submit
    â†“
ingestion_submit_view()
    â”œâ”€ Parse document_ids
    â”œâ”€ Queue ingestion tasks
    â””â”€ Returns task IDs
```

**KEY FINDING**:

- âœ… Completely independent from crawler
- âœ… Does NOT use crawler code paths

---

## ğŸ”Œ Integration Points (MUST NOT BREAK)

### **1. crawl_selected() - Critical Integration Point**

**Used By**:

- âœ… Web Search â†’ "Ingest Selected" button
- âœ… Through `web_search_ingest_selected()`

**Location**: `ai_core/views.py:crawl_selected()`

**Contract** (Verified):

```python
@require_POST
def crawl_selected(request):
    # Expects JSON body:
    # {
    #   "urls": ["url1", "url2"],
    #   "workflow_id": "web-search-ingestion",
    #   "mode": "live" | "archive",
    #   "collection_id": "...",
    # }
    
    # Returns JSON:
    # {
    #   "task_ids": [...],
    #   "status": "accepted" | "completed",
    #   ...
    # }
```

**Current Implementation** (Verified in code):

```python
crawl_selected(request)
    â†“
_prepare_request(request) â†’ meta
    â†“
json.loads(request.body) â†’ data
    â†“
Build CrawlerRunRequest from data.urls
    â†“
run_crawler_runner(meta, request_model, lifecycle_store, graph_factory)
    â†“
Returns JsonResponse with task_ids
```

**âš ï¸ CRITICAL**: Any changes to crawler architecture must:

- âœ… Keep `crawl_selected()` API contract
- âœ… Accept same JSON body structure
- âœ… Return same JSON response structure
- âœ… Support `mode` parameter (live/archive)

---

### **2. run_crawler_runner() - Internal Integration**

**Used By**:

- âœ… `crawl_selected()` (Web Search path)
- âœ… `crawler_submit_view()` (Crawler Tab path)

**Location**: `ai_core/services/crawler_runner.py:run_crawler_runner()`

**Contract** (Verified):

```python
def run_crawler_runner(
    *,
    meta: dict[str, Any],
    request_model: CrawlerRunRequest,
    lifecycle_store: object | None,
    graph_factory: Callable[[], object] | None = None,
) -> CrawlerRunnerCoordinatorResult:
    # Returns:
    # CrawlerRunnerCoordinatorResult(
    #     payload={"task_ids": [...], ...},
    #     status_code=200 | 202,
    #     idempotency_key=...
    # )
```

**Current Flow** (Verified):

```python
run_crawler_runner(...)
    â†“
For each URL in request_model.origins:
    â”œâ”€ build_crawler_state() â†’ state dict
    â”œâ”€ CrawlerWorker.process() â† Calls parallel registration!
    â”‚  â””â”€ _register_document() â† Creates document in DB
    â””â”€ Publish to Celery OR run graph inline
```

**âš ï¸ PROBLEM**: This is where parallel registration happens!

---

## ğŸš¨ What Our Redesign MUST Preserve

### **Backwards Compatibility Requirements**

1. **âœ… crawl_selected() API**:
   - Same endpoint path
   - Same JSON contract
   - Same response structure
   - Support `mode` parameter

2. **âœ… Web Search Integration**:
   - "Ingest Selected" button works
   - Results render correctly
   - Status panel updates

3. **âœ… Crawler Tab**:
   - Form submission works
   - Status updates work
   - Can specify URLs manually

4. **âœ… Mode Parameter**:
   - `mode: "live"` and `mode: "archive"` both work
   - Our new modes (rag/archive/ephemeral) should map correctly

---

## ğŸ¯ Safe Migration Strategy

### **Option 1: Non-Breaking Refactor** (Recommended)

**Step 1**: Keep `crawl_selected()` as-is (facade)

```python
@require_POST
def crawl_selected(request):
    """
    UNCHANGED facade - maintains API contract.
    Internal implementation can change!
    """
    # ... existing validation ...
    
    # NEW: Delegate to new coordinator
    coordinator = CrawlerCoordinator()
    result = coordinator.ingest_direct(
        urls=urls,
        mode=mode,  # Map mode parameter
        tenant_id=meta["tenant_id"],
        workflow_id=workflow_id,
        collection_id=collection_id,
        ...
    )
    
    # Return same response format
    return JsonResponse({
        "task_ids": result.task_ids,
        "status": "accepted",
        ...
    })
```

**Step 2**: New coordinator handles logic

```python
class CrawlerCoordinator:
    def ingest_direct(self, urls, mode, ...):
        """
        NEW internal implementation.
        Maps old 'mode' to new modes:
        - "live" â†’ "rag"
        - "archive" â†’ "archive"
        """
        new_mode = "rag" if mode == "live" else "archive"
        task = ingest_urls.delay(
            session_id=None,  # No HITL for direct calls
            urls=urls,
            mode=new_mode,
            ...
        )
        return IngestResult(task_ids=[task.id])
```

**Benefits**:

- âœ… No API changes
- âœ… Web Search keeps working
- âœ… Cleaner internal architecture
- âœ… Can add HITL later without breaking anything

---

### **Option 2: Add HITL Alongside** (Future Enhancement)

**Keep existing flow**:

```
Web Search â†’ "Ingest Selected" â†’ crawl_selected() â†’ works as before
```

**Add new HITL flow** (parallel, not replacing):

```
New UI â†’ "Fetch for Review" â†’ NEW endpoint â†’ HITL flow
```

**Benefits**:

- âœ… Zero risk to existing features
- âœ… Can test HITL separately
- âœ… Gradual migration

---

## ğŸ“‹ Verification Checklist

Before deploying any changes, verify:

### **Web Search Tab**

- [ ] Search returns results
- [ ] "Ingest Selected" button appears
- [ ] Clicking "Ingest Selected" triggers ingestion
- [ ] Status panel shows progress
- [ ] Mode parameter (live/archive) works

### **Crawler Tab**

- [ ] Can enter URLs manually
- [ ] "Start Crawl" triggers processing
- [ ] Status area shows results
- [ ] Mode parameter works

### **API Contracts**

- [ ] `crawl_selected()` accepts same JSON
- [ ] `crawl_selected()` returns same JSON structure
- [ ] `run_crawler_runner()` signature unchanged (or backwards compatible)

### **Error Cases**

- [ ] Invalid URLs handled gracefully
- [ ] Missing tenant_id shows error
- [ ] Empty URL list shows error

---

## ğŸ¯ Conclusion

**What we VERIFIED**:

1. âœ… `/rag-tools/` has 3 tabs (Web Search, Crawler, Ingestion)
2. âœ… Web Search â†’ "Ingest Selected" â†’ calls `crawl_selected()`
3. âœ… `crawl_selected()` is the critical integration point
4. âœ… Current `mode` parameter: "live" or "archive"
5. âœ… Parallel registration happens in `run_crawler_runner()`

**What we MUST preserve**:

1. âœ… `crawl_selected()` API contract
2. âœ… Web Search â†’ Ingest Selected flow
3. âœ… Mode parameter support
4. âœ… Response structure

**Safe approach**:

- âœ… Keep `crawl_selected()` as facade
- âœ… Refactor internal implementation
- âœ… Map old modes to new modes
- âœ… Add HITL as separate feature (later)

**NO DRIFT!** All existing `/rag-tools/` features will continue working! ğŸ¯
